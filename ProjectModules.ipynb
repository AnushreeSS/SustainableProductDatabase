{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dcaff6",
   "metadata": {},
   "source": [
    "# Data Pipeline Modules\n",
    "\n",
    "In this notebook, all the required classes to buils a sustainable product database is avaiable.\n",
    "\n",
    "For Data Preprocessing, below are classes can be used \n",
    "- ColumnDropper\n",
    "- RowDropper\n",
    "- StringCleaner\n",
    "\n",
    "For Keyword Extraction using TF-IDF, __KeywordExtractor__ class can be used. This class includes all the cutomize functions defined to extract keyword accurately based on a specfic vocabulary. \n",
    "\n",
    "For mapping keywords to specific sustainbility factors using the ontology data, __KeywordMapper__ class can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701f6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation, digits\n",
    "from pandas import isna, DataFrame, pivot_table, read_sql_query\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import sqlite3\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21d2b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"custom_stopwords.txt\", \"r\") as f:\n",
    "    ENGLISH_STOP_WORDS = f.read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eef309a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDropper():\n",
    "    \"\"\"Given a list of columns, this class is used to drop or retain those columns\"\"\"\n",
    "    def __init__(self, columns:list, should_drop:bool=False):\n",
    "        \"\"\" If should_drop is False, then the given columns will be retained and rest will be dropped\n",
    "            If should_drop is True, then the given columns will be dropped and rest will be retained\"\"\"\n",
    "        self.columns = columns\n",
    "        self.should_drop = should_drop\n",
    "    \n",
    "    def fit(self, X:DataFrame, y=None):\n",
    "        # No additional parameter required\n",
    "        return self \n",
    "\n",
    "    def transform(self, X:DataFrame):\n",
    "        # Filter all the columns from the given dataframe that are present \n",
    "        # in the columns provided during initializing the class.\n",
    "        drop_columns = [col for col in X.columns if col in self.columns]\n",
    "        \n",
    "        # If include=True, then the filtered columns have to be retained.\n",
    "        # Therefore, remaining columns have to be dropped\n",
    "        if not self.should_drop:\n",
    "            drop_columns = X.columns.difference(drop_columns)\n",
    "        X = X.drop(drop_columns, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca57879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowDropper():\n",
    "    \"\"\"Given a list of columns, this class is used to drop rows with np.nan in any of the listed columns\n",
    "       and removes any duplicate rows in the subset of given columns\"\"\"\n",
    "    def __init__(self, columns:list=None):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def fit(self, X:DataFrame, y=None):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X:DataFrame):\n",
    "        # If columns are not provided, all the columns are considered\n",
    "        if not self.columns:\n",
    "            self.columns = X.columns\n",
    "        X = X.dropna(subset=self.columns, how=\"any\")\n",
    "        X = X.drop_duplicates(subset=self.columns, keep=\"last\").reset_index(drop=True)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da2ddf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StringCleaner():\n",
    "    \"\"\"\n",
    "    Given a list of columns, this class is used clean the string in the listed columns with following steps:\n",
    "            1. Convert string to lower case\n",
    "            2. Remove HTML encoded characters (characters starting with &)\n",
    "            3. Remove XML tags (characters between <>)\n",
    "            4. Remove white spaces\n",
    "            5. Remove punctuations and digits\n",
    "            6. Replace empty strings with np.nan\n",
    "    \"\"\"\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.translator = str.maketrans(punctuation+digits, ' '*len(punctuation+digits))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self \n",
    "        \n",
    "    def transform(self, X:DataFrame):\n",
    "        for column in self.columns:\n",
    "            X[column] = X[column].astype(str)\n",
    "            # 1. Convert string to lower case\n",
    "            X[column] = X[column].str.lower()\n",
    "            # 2. Remove HTML encoded characters (characters starting with &)\n",
    "            X[column] = X[column].apply(lambda s: re.sub('&\\w+',' ',str(s)) if not isna(s) else s)\n",
    "            # 3. Remove XML tags (characters between <>)\n",
    "            X[column] = X[column].apply(lambda s: re.sub('<\\w*>',' ',str(s)) if not isna(s) else s)\n",
    "            # 4. Remove white spaces\n",
    "            X[column] = X[column].apply(lambda s: re.sub(r'\\\\\\w', ' ',str(s)) if not isna(s) else s)\n",
    "            X[column] = X[column].apply(lambda s: re.sub(r'\\s+',' ',str(s)) if not isna(s) else s)\n",
    "            # 5. Remove punctuations and digits\n",
    "            X[column] = X[column].apply(lambda s: s.translate(self.translator) if not isna(s) else s)\n",
    "            X[column] = X[column].apply(lambda s: re.sub('\\s+',' ',str(s)) if not isna(s) else s)\n",
    "            # 6. Replace empty strings with np.nan\n",
    "            X[column] = X[column].str.strip()\n",
    "            X[column] = X[column].replace('', np.nan)\n",
    "            X[column] = X[column].replace(' ', np.nan)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b0d464",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordExtractor():\n",
    "    product_tfidf = None\n",
    "    ontology_tf = None\n",
    "    trigram_mapping = None\n",
    "    custom_vocab = []\n",
    "    tfidfVectorizer = None\n",
    "    def __init__(self, stop_words='english', ngram_range=(2,3), word_window_length:int=4):\n",
    "        if isinstance(stop_words, str) and stop_words.lower()=='english':\n",
    "            # Use ENGLISH STOP WORDs by default\n",
    "            self.stop_words = ENGLISH_STOP_WORDS\n",
    "        else:\n",
    "            self.stop_words = stop_words\n",
    "        self.ngram_range = ngram_range\n",
    "        self.word_window_length = word_window_length\n",
    "    \n",
    "    def extract_keywords(self, ontology_data, product_data):\n",
    "        # Create vocabulary from ontology data\n",
    "        self.custom_vocab = self.create_vocab(ontology_data)\n",
    "        # Compute TF-IDF scores\n",
    "        self.product_tfidf = self.compute_tfidf(product_data)\n",
    "        # Remove redundant bigrams\n",
    "        self.product_tfidf = self.eliminate_redundant_bigram()\n",
    "        # Computing term-frequency (TF) for ontology data\n",
    "        self.ontology_tf = self.compute_tf(ontology_data)\n",
    "        # Remove any partially overlapping match\n",
    "        self.ontology_tf = self.eliminate_partial_mapping_tf()\n",
    "        return self.product_tfidf, self.ontology_tf\n",
    "    \n",
    "    def custom_ngram_generator(self, s: str):\n",
    "        # Get all words in a string\n",
    "        words = s.split()\n",
    "        n_grams = []\n",
    "        # Remove stop words or single letter words and stem common variations of a word containing 's' or 'ing'\n",
    "        words = [re.sub(r'(s|ing)$','',word) for word in words if (len(word)>1 and word not in self.stop_words)]\n",
    "        for i in range(len(words)):\n",
    "            # ngram_range = (2,3)\n",
    "            min_ngram = self.ngram_range[0]\n",
    "            max_ngram = self.ngram_range[1]\n",
    "            for j in range(min_ngram, max_ngram+1):\n",
    "                # Get all bigram and trigrams in a given window\n",
    "                n_grams+=[' '.join(n_gram) for n_gram in combinations(words[i:i+self.word_window_length], j)]\n",
    "        return list(set(n_grams))\n",
    "    \n",
    "    def create_vocab(self, data):\n",
    "        self.custom_vocab = []\n",
    "        for i in data:\n",
    "            self.custom_vocab+=self.custom_ngram_generator(i)\n",
    "        self.custom_vocab = np.array(sorted(set(self.custom_vocab)))\n",
    "        return self.custom_vocab\n",
    "    \n",
    "    def compute_tfidf(self, data):\n",
    "        # Create TF-IDF vectorizer if not initialized already\n",
    "        if not self.tfidfVectorizer:\n",
    "            if len(self.custom_vocab)!=0:\n",
    "                self.tfidfVectorizer = TfidfVectorizer(vocabulary=self.custom_vocab,\n",
    "                                                       analyzer=self.custom_ngram_generator)\n",
    "            else:\n",
    "                self.tfidfVectorizer = TfidfVectorizer(analyzer=self.custom_ngram_generator)\n",
    "            self.tfidfVectorizer.fit(data)\n",
    "        # Compute TF-IDF for product descriptions\n",
    "        self.product_tfidf = self.tfidfVectorizer.transform(data)\n",
    "        return self.product_tfidf\n",
    "    \n",
    "    def compute_tf(self, data):\n",
    "        if len(self.custom_vocab)!=0:\n",
    "            temp_tfidfVectorizer = TfidfVectorizer(vocabulary=self.custom_vocab,\n",
    "                                              analyzer=self.custom_ngram_generator,\n",
    "                                              use_idf=False)\n",
    "        else:\n",
    "            temp_tfidfVectorizer = TfidfVectorizer(analyzer=self.custom_ngram_generator,\n",
    "                                              use_idf=False)\n",
    "        self.ontology_tf = temp_tfidfVectorizer.fit_transform(data)\n",
    "        return self.ontology_tf\n",
    "    \n",
    "    def eliminate_partial_mapping_tf(self):\n",
    "        self.ontology_tf = self.ontology_tf.toarray()\n",
    "        for col in range(self.ontology_tf.shape[1]):\n",
    "            # Check if the column has a complete match (=1)\n",
    "            if (self.ontology_tf[:,col]==1).sum()!=0:\n",
    "                # Get all partially mapping indices (those with value < 1)\n",
    "                partial_mapping_indices = np.where(self.ontology_tf[:,col]<1)\n",
    "                if len(partial_mapping_indices)!=0:\n",
    "                    # Reset score to 0 to the partially mtaching indices\n",
    "                    self.ontology_tf[partial_mapping_indices,col]=0\n",
    "        self.ontology_tf = csr_matrix(self.ontology_tf)\n",
    "        return self.ontology_tf\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.custom_vocab\n",
    "    \n",
    "    def create_trigram_mapping(self):\n",
    "        self.trigram_mapping = {}\n",
    "        for idx, feature in enumerate(self.custom_vocab):\n",
    "            if len(feature.split())==3:\n",
    "                bigrams = combinations(feature.split(), 2)\n",
    "                self.trigram_mapping[idx] = [np.where(self.custom_vocab==' '.join(bigram))[0][0] \n",
    "                                             for bigram in bigrams]\n",
    "    \n",
    "    def eliminate_redundant_bigram(self):\n",
    "        if not self.trigram_mapping:\n",
    "            self.create_trigram_mapping()\n",
    "        \n",
    "        # Redundant bigram elimination\n",
    "        self.product_tfidf = self.product_tfidf.toarray()\n",
    "        for row_idx in np.unique(self.product_tfidf.nonzero()[0]):\n",
    "            feature_indices = self.product_tfidf[row_idx].nonzero()[0]\n",
    "            if len(feature_indices)<=2:\n",
    "                continue\n",
    "            # Filter trigrams\n",
    "            feature_indices = list(filter(lambda feature_idx: feature_idx in self.trigram_mapping, feature_indices))\n",
    "            for feature_idx in feature_indices:\n",
    "                # Remove bigram associated with the trigram\n",
    "                bigram_indices =  self.trigram_mapping[feature_idx]\n",
    "                self.product_tfidf[row_idx][np.array(bigram_indices)] = np.zeros(len(bigram_indices))\n",
    "        self.product_tfidf = csr_matrix(self.product_tfidf)\n",
    "        return self.product_tfidf\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc875b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordsMapper():\n",
    "    def __init__(self, keyword_extractor:KeywordExtractor):\n",
    "        self.ontology_tf = keyword_extractor.ontology_tf\n",
    "        self.product_tfidf = keyword_extractor.product_tfidf\n",
    "        \n",
    "    def map_keywords(self):\n",
    "        # Matrix Multiplication\n",
    "        self.importance = np.dot(self.product_tfidf, self.ontology_tf.T)\n",
    "        self.importance = self.importance.tocoo()\n",
    "        # Converting to Data Frame\n",
    "        self.mapping = DataFrame(zip(self.importance.row, self.importance.col, self.importance.data))\n",
    "        self.mapping.columns = [\"product_idx\", \"onto_idx\", \"imp_score\"]\n",
    "        return self.mapping\n",
    "    \n",
    "    def integrate_ontology(self, ontology_data):\n",
    "    # If numnber of unique ontology indices in the mapping is more than the given ontology data, then error out\n",
    "        if len(self.mapping[\"onto_idx\"].unique())>ontology_data.shape[0]:\n",
    "            raise Exception(\"Invalid Ontology Data (Size Mismatch)\")\n",
    "        self.ontology_data = ontology_data\n",
    "        # Left join the matrix with ontology data by index\n",
    "        self.mapping = self.mapping.merge(ontology_data, how=\"left\", left_on=\"onto_idx\", right_index=True)\n",
    "        # Multiply importance score with association value from the ontology data\n",
    "        self.mapping[\"association_imp_score\"] = self.mapping[\"association\"]*self.mapping[\"imp_score\"]\n",
    "        return self.mapping\n",
    "    \n",
    "    def aggregate_mapping(self, by:str=\"preference category\", conditional:bool=None):\n",
    "        filtered_df = self.mapping\n",
    "        if conditional!=None:\n",
    "            # Filter by the given conditional value\n",
    "            filtered_df = filtered_df[filtered_df.conditional==conditional]\n",
    "        final_mapping = pivot_table(filtered_df, \n",
    "                                    values=[\"association_imp_score\"], \n",
    "                                    index=[\"product_idx\"], \n",
    "                                    columns=[by], \n",
    "                                    aggfunc=np.average)\n",
    "        return final_mapping\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7741ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseWriter():\n",
    "    # table to column mapping\n",
    "    column_dict = {\"ontology_data\": [\"idx\", \"tag\", \"sustainability_preference\", \n",
    "                                     \"preference_category\", \"association\", \"conditional\"],\n",
    "                   \"vocabulary\": [\"idx\", \"vocab\"],\n",
    "                   \"product_data\": [\"idx\", \"source\", \"product_code\", \"product_title\",\n",
    "                                    \"product_description\", \"product_category\", \"brand\", \"price\"],\n",
    "                   \"product_ontology_mapping\": [\"source\", \"product_idx\", \"onto_idx\", \"imp_score\"],\n",
    "                   \"product_keywords\": [\"source\", \"product_idx\", \"vocab_idx\"]}\n",
    "    def __init__(self, db_file_path: str=\"sustainable_product_db.db\"):\n",
    "        self.db_file_path = db_file_path\n",
    "        \n",
    "    def create_tables(self, sql_script_path: str=\"sustainable_product_db_tables.sql\"):\n",
    "        # create tables using an SQL script\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_file_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                script = open(sql_script_path, \"r\").read()\n",
    "                cursor.executescript(script)\n",
    "                print(\"Successfully created tables\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create tables!!!\")\n",
    "            raise e\n",
    "        \n",
    "    def insert_data(self, df, table_name:str, include_index:bool=True):\n",
    "        # Insert a datafrme values into a table. If include index is true then, index is also inserted\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_file_path) as conn:\n",
    "                query = f'insert or replace into {table_name} ({\", \".join(self.column_dict[table_name])}) values ({\", \".join([\"?\"]*len(self.column_dict[table_name]))});'\n",
    "                if include_index:\n",
    "                    values = list(df[self.column_dict[table_name][1:]].itertuples(index=True))\n",
    "                else:\n",
    "                    values = list(df[self.column_dict[table_name]].itertuples(index=False))\n",
    "                conn.cursor().executemany(query, values)\n",
    "                print(f\"Successfully inserted (or updated) {table_name} table\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(\"Failed to insert data into table!\")\n",
    "            raise e\n",
    "    \n",
    "    def execute_script(self, query_script:str=\"generate_pivot_query.sql\", should_generate_query:bool=False):\n",
    "        # Execute an SQL script. If generate query is set to true, the query is generated, then executed\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_file_path) as conn:\n",
    "                cursor = conn.cursor()\n",
    "                script = open(query_script, \"r\").read()\n",
    "                if should_generate_query:\n",
    "                    script = cursor.execute(str(script)).fetchone()[0]\n",
    "                    print(f\"Pivot table query generated: {script}\")\n",
    "                cursor.executescript(script)\n",
    "                print(f\"Script {query_script} executed succesfully!\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to auto insert into sustainable mapping\")\n",
    "            raise e\n",
    "            \n",
    "    def read_table(self, table_name):\n",
    "        # Read a given table and return it as a data frame\n",
    "        try:\n",
    "            with sqlite3.connect(self.db_file_path) as conn:\n",
    "                query = f\"select * from {table_name}\"\n",
    "                df = read_sql_query(query, conn)\n",
    "                print(f\"Successfully loaded data from {table_name}!\")\n",
    "                return df\n",
    "        except Exception as e:\n",
    "            print(f\"Failed fetch data from {table_name}\")\n",
    "            raise e\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c81055",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
